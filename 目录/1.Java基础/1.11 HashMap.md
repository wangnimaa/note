hashmap 的负载因子为什么是0.75？

那么，为什么选择0.75呢？背后有什么考虑？为什么不是1，不是0.8？不是0.5，而是0.75呢？
在JDK的官方文档中，有这样一段描述描述：

As a general rule, the default load factor (.75) offers a good tradeoff between time and space costs. 
Higher values decrease the space overhead but increase the lookup cost (reflected in most of the operations of the HashMap class, 
including get and put).

大概意思是：一般来说，默认的负载因子(0.75)在时间和空间成本之间提供了很好的权衡。更高的值减少了空间开销，但增加了查找成本
(反映在HashMap类的大多数操作中，包括get和put)。

试想一下，如果我们把负载因子设置成1，容量使用默认初始值16，那么表示一个HashMap需要在"满了"之后才会进行扩容。
那么在HashMap中，最好的情况是这16个元素通过hash算法之后分别落到了16个不同的桶中，否则就必然发生哈希碰撞。而且随着元素越多，
哈希碰撞的概率越大，查找速度也会越低。

理论上我们认为负载因子不能太大，不然会导致大量的哈希冲突，也不能太小，那样会浪费空间。
通过一个数学推理，测算出这个数值在0.7左右是比较合理的。
那么，为什么最终选定了0.75呢？

临界值（threshold） = 负载因子（loadFactor） * 容量（capacity）。
根据HashMap的扩容机制，他会保证capacity的值永远都是2的幂。
那么，为了保证负载因子（loadFactor） * 容量（capacity）的结果是一个整数，这个值是0.75(3/4)比较合理，因为这个数和任何2的
幂乘积结果都是整数。

总结
HashMap是一种K-V结构，为了提升其查询及插入的速度，底层采用了链表的数组这种数据结构实现的。
但是因为在计算元素所在的位置的时候，需要使用hash算法，而HashMap采用的hash算法就是链地址法。这种方法有两个极端。
如果HashMap中哈希冲突概率高，那么HashMap就会退化成链表（不是真的退化，而是操作上像是直接操作链表），而我们知道，链表最大的缺点就是查询速度比较慢，他需要从表头开始逐一遍历。
所以，为了避免HashMap发生大量的哈希冲突，所以需要在适当的时候对其进行扩容。
而扩容的条件是元素个数达到临界值时。HashMap中临界值的计算方法：
临界值（threshold） = 负载因子（loadFactor） * 容量（capacity）

其中负载因子表示一个数组可以达到的最大的满的程度。这个值不宜太大，也不宜太小。
loadFactor太大，比如等于1，那么就会有很高的哈希冲突的概率，会大大降低查询速度。
loadFactor太小，比如等于0.5，那么频繁扩容没，就会大大浪费空间。
所以，这个值需要介于0.5和1之间。根据数学公式推算。这个值在log(2)的时候比较合理。
另外，为了提升扩容效率，HashMap的容量（capacity）有一个固定的要求，那就是一定是2的幂。
所以，如果loadFactor是3/4的话，那么和capacity的乘积结果就可以是一个整数。
所以，一般情况下，我们不建议修改loadFactor的值，除非特殊原因。
比如我明确的知道我的Map只存5个kv，并且永远不会改变，那么可以考虑指定loadFactor。
但是其实我也不建议这样用。我们完全可以通过指定capacity达到这样的目的。
