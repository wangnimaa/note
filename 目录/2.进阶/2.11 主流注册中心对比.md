
                      Nacos	                          Eureka	                    Consul	         CoreDNS	                  Zookeeper
  
  一致性协议   	     CP+AP	                           AP	                        CP	               —	                      CP

  健康检查	    TCP/HTTP/MYSQL/Client                 Beat	                      Client Beat       TCP/HTTP/gRPC/Cmd	— 	    Keep Alive
  
  负载均衡策略	权重/ metadata/Selector	              Ribbon	                    Fabio	            RoundRobin	                —
  
  雪崩保护	          有	                             有	                        无	                无	                        无

  自动注销实例	    支持	                            支持	                      不支持	            不支持	                    支持
  
  访问协议	        HTTP/DNS	                        HTTP	                      HTTP/DNS	         DNS	                     TCP
  
  监听支持	        支持	                            支持	                      支持	             不支持	                  支持
      
  多数据中心	       支持	                             支持	                       支持	              不支持	                   不支持
  
  跨注册中心同步	   支持	                             不支持	                    支持	             不支持	                  不支持
  
  SpringCloud集成	  支持	                            支持	                      支持	             支持	                     不支持
  
  Dubbo集成	        支持	                            不支持	                     不支持	           不支持	                  支持
  
  K8S集成	          支持	                            不支持	                     支持	              支持	                    不支持



Zookeeper  	AP

ZK在设计之初就遵循CP原则，任何时候对ZK的访问请求能得到一致的数据结果，同时具有分区容错性，但不能保证每次请求都是可达的。
Zookeeper使用的ZAB协议，由于是单点写，在集群扩展性上不具备优势。
在使用ZK获取服务列表时，如果此时的ZK集群中的Leadr宕机了，该集群就要进行Leader的选举，又或者ZK集群中半数以上服务器节点不可用
（如有3个节点，如果节点1检测到节点3挂了，节点2也检测到节点3挂了，那这个节点才算是真的挂了），那么将无法处理该请求。所以ZK不能
保证服务的可用性。
双机房容灾，基于Leader写的协议不做改造是无法支持的，这意味着Zookeeper不能在没有人工干预的情况下做到双机房容灾。在单机房断网情况下，
使机房内服务可用并不难，难的是如何在断网恢复后做数据聚合，Zookeeper的单点写模式就会有断网恢复后的数据对账问题。
Zookeeper在写性能上似乎能达到上万的TPS，这得益于Zookeeper精巧的设计，不过这显然是因为有一系列的前提存在。首先Zookeeper的写逻辑就是进行K-V的写入，内部没有聚合；其次Zookeeper舍弃了服务发现的基本功能如健康检查、友好的查询接口，它在支持这些功能的时候，显然需要增加一些逻辑，甚至弃用现有的数据结构；最后，Paxos协议本身就限制了Zookeeper集群的规模，3、5个节点是不能应对大规模的服务订阅和查询的。
Zookeeper的容量，从存储节点数来说，可以达到百万级别。不过如上面所说，这并不代表容量的全部，当大量的实例上下线时，Zookeeper的表现并不稳定，同时在推送机制上的缺陷，会引起客户端的资源占用上升，从而性能急剧下降。

Nacos

Nacos因为要支持多种服务类型的注册，并能够具有机房容灾、集群扩展等必不可少的能力。支持AP和CP两种一致性协议并存。
Nacos支持两种模式的部署，一种是和Eureka一样的AP协议的部署，这种模式只支持临时实例，可以完美替代当前的Zookeeper、Eureka，并支持机房容灾。另一种是支持持久化实例的CP模式，这种情况下不支持双机房容灾。
Nacos目前支持临时实例使用心跳上报方式维持活性，发送心跳的周期默认是5秒，Nacos服务端会在15秒没收到心跳后将实例设置为不健康，在30秒没收到心跳时将这个临时实例摘除。

Eureka

Eureka的部署模式天然支持多机房容灾，因为Eureka采用的是纯临时实例的注册模式：不持久化、所有数据都可以通过客户端心跳上报进行补偿。
Eureka在服务实例规模在5000左右的时候，就已经出现服务不可用的问题，甚至在压测的过程中，如果并发的线程数过高，就会造成Eureka crash。不过如果服务规模在1000上下，几乎目前所有的注册中心都可以满足。毕竟我们看到Eureka作为SpringCloud的注册中心，在国内也没有看到很广泛的对于容量或者性能的问题报告。
Zookeeper和Eureka都实现了一种TTL的机制，就是如果客户端在一定时间内没有向注册中心发送心跳，则会将这个客户端摘除。Eureka做的更好的一点在于它允许在注册服务的时候，自定义检查自身状态的健康检查方法。这在服务实例能够保持心跳上报的场景下，是一种比较好的体验，在Dubbo和SpringCloud这两大体系内，也被培养成用户心智上的默认行为。
